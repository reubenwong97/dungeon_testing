{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5999f628",
   "metadata": {},
   "source": [
    "# Setup ML Agents Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ccedc65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:37.626414Z",
     "start_time": "2021-10-22T15:20:37.178940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev.x86_64\n",
      "Mono path[0] = '/home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Managed'\n",
      "Mono config path = '/home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'lib_burst_generated.so'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Initialize engine version: 2020.3.16f1 (049d6eca3c44)\n",
      "[Subsystems] Discovering subsystems at path /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; threaded=0\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "- Completed reload, in  0.046 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader GUI/Text Shader shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "UnloadTime: 1.071420 ms\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libcoreclr.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libcoreclr.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libcoreclr.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libSystem.dylib\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libSystem.dylib.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libSystem.dylib\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libcoreclr.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libcoreclr.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libcoreclr.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libSystem.dylib\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libSystem.dylib.so\n",
      "Fallback handler could not load library /home/reuben/dungeon_testing/../dungeon_nodev/dungeon_escape_nondev_Data/Mono/libSystem.dylib\n"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "# -----------------\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../dungeon_nodev/dungeon_escape_nondev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a894eef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:37.713872Z",
     "start_time": "2021-10-22T15:20:37.630214Z"
    }
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891315c",
   "metadata": {},
   "source": [
    "# Behavior Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50911b",
   "metadata": {},
   "source": [
    "## Get Behavior Specs from Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9006cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:37.722848Z",
     "start_time": "2021-10-22T15:20:37.716504Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BehaviorSpec(observation_specs=[ObservationSpec(shape=(10,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RBSensor'), ObservationSpec(shape=(360,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='StackingSensor_size3_RayPerceptionSensor'), ObservationSpec(shape=(1,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size1')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(7,)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25728994",
   "metadata": {},
   "source": [
    "## Get Observation Space from Behavior Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63458bcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:38.344585Z",
     "start_time": "2021-10-22T15:20:38.338593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  3\n",
      "Is there a visual observation ? False\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(spec.observation_specs))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "# Visual observation have 3 dimensions: Height, Width and number of channels\n",
    "vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "print(\"Is there a visual observation ?\", vis_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd9dbe",
   "metadata": {},
   "source": [
    "## Get Action Space from Behavior Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "807f0a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:38.508005Z",
     "start_time": "2021-10-22T15:20:38.501242Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 discrete actions\n",
      "Action number 0 has 7 different options\n"
     ]
    }
   ],
   "source": [
    "# Is the Action continuous or multi-discrete ?\n",
    "if spec.action_spec.continuous_size > 0:\n",
    "    print(f\"There are {spec.action_spec.continuous_size} continuous actions\")\n",
    "if spec.action_spec.is_discrete():\n",
    "    print(f\"There are {spec.action_spec.discrete_size} discrete actions\")\n",
    "\n",
    "\n",
    "# How many actions are possible ?\n",
    "#print(f\"There are {spec.action_size} action(s)\")\n",
    "\n",
    "# For discrete actions only : How many different options does each action has ?\n",
    "if spec.action_spec.discrete_size > 0:\n",
    "    for action, branch_size in enumerate(spec.action_spec.discrete_branches):\n",
    "        print(f\"Action number {action} has {branch_size} different options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21604c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:38.582340Z",
     "start_time": "2021-10-22T15:20:38.578460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.action_spec.discrete_branches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df331765",
   "metadata": {},
   "source": [
    "## Learning About the Observations for Agents\n",
    "---\n",
    "As seen from the cell below, there are 3 sources of observations coming from the `RBSensor`, `RayPerceptionSensor` and `VectorSensor`. The observations have a `batch_size` of 36, stemming from the 3 agents, replicated over 12 learning environments.\n",
    "\n",
    "* `RBSensor` refers to `RigidBodySensor`, which within the Unity editor tells me this is not useful because this model lacks joints and will always be identity values, can actually be removed.\n",
    "* The other 2 are useful, but need to think of how to represent them, do I just concatenate them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2ea588b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:38.754589Z",
     "start_time": "2021-10-22T15:20:38.747829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 10)\n",
      "(36, 360)\n",
      "(36, 1)\n",
      "torch.Size([36, 361])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "decision_steps, terminal_steps = env.get_steps(\"DungeonEscape?team=0\")\n",
    "\n",
    "# observations coming from 3 sources, in batches of 3 * 12 (replicated envs) = 36\n",
    "print(decision_steps.obs[0].shape)  # rb_sensor, DO NOT USE\n",
    "print(decision_steps.obs[1].shape)  # ray perception sensor\n",
    "print(decision_steps.obs[2].shape)  # vector sensor\n",
    "print(th.cat((th.tensor(decision_steps.obs[1]), th.tensor(\n",
    "    decision_steps.obs[2])), 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e4979",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396be0fb",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Using NamedTuples could be a neater method of doing things. Probably can use it for the single agent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3b4fd7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.000287Z",
     "start_time": "2021-10-22T15:20:38.993737Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List, Deque, Dict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    \"\"\"\n",
    "    An experience contains the data of one Agent transition.\n",
    "    - Observation\n",
    "    - Action\n",
    "    - Reward\n",
    "    - Done flag\n",
    "    - Next Observation\n",
    "    \"\"\"\n",
    "\n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray # Check on the actual actions required by Unity\n",
    "    reward: float\n",
    "    done: bool\n",
    "    next_obs: np.ndarray\n",
    "\n",
    "# Pretty cool, these aren't instantiations but like composite typing\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience] # small enough so I am not afraid of exceeding\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = Deque[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2917bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.090588Z",
     "start_time": "2021-10-22T15:20:39.078036Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        \"\"\"\n",
    "        To deal with the potential changing number of agents, each with valid\n",
    "        observations -> just store the [36, 361] observations as 36 seperate\n",
    "        entries, and the 36 actions also as seperate entries... I think...\n",
    "        \"\"\"\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "#         self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "#                                      dtype=np.float32)\n",
    "#         self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "#                                          dtype=np.float32)\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape),\n",
    "                                         dtype=np.float32)\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb51a6",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "431d5511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.249577Z",
     "start_time": "2021-10-22T15:20:39.245642Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a37aab",
   "metadata": {},
   "source": [
    "### Caveat for Linear Layer\n",
    "As per [pytorch docs](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), input is of shape \\[N, *, input_dims\\], where * means any number of additional dimensions. So it will implicitly handle our observations of \\[32, 36, 361\\] by just passing 361 as input dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0457e8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.422278Z",
     "start_time": "2021-10-22T15:20:39.413914Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Currently just boilerplate code, to tweak architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_size1: int,\n",
    "                 hidden_size2: int,\n",
    "                 output_size: int,\n",
    "                 args: SimpleNamespace):\n",
    "        \"\"\"\n",
    "        Input to the network will be of shape [B, n_agents, obs_size]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.fc1 = nn.Linear(input_shape, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.device = th.device(args.device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.fc3(x)\n",
    "\n",
    "        # return q-values for actions\n",
    "        return q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdda12",
   "metadata": {},
   "source": [
    "## Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7816a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.585063Z",
     "start_time": "2021-10-22T15:20:39.581774Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1963cb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.696320Z",
     "start_time": "2021-10-22T15:20:39.670577Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, args: SimpleNamespace, env: UnityEnvironment):\n",
    "        self.args = args\n",
    "        self.device = th.device(args.device)\n",
    "        self.epsilon = args.epsilon\n",
    "        self.eps_min = args.eps_min\n",
    "        self.eps_dec = args.eps_dec\n",
    "        self.replace_target_cnt = args.replace\n",
    "        self.batch_size = args.batch_size\n",
    "        self.gamma = args.gamma\n",
    "\n",
    "        # environment related information\n",
    "        self.env = env\n",
    "        self.behavior_name = list(env.behavior_specs.keys())[0]\n",
    "        self.spec = env.behavior_specs[self.behavior_name]\n",
    "        self.n_actions = self.spec.action_spec.discrete_branches[0]\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        # RL related\n",
    "        self.memory: Buffer = deque([], maxlen=args.mem_size)\n",
    "        self.q_eval = DQN(args.input_shape, args.hidden_size1,\n",
    "                          args.hidden_size2, args.output_size, args)\n",
    "        # target network, perhaps can deepcopy?\n",
    "        self.q_next = DQN(args.input_shape, args.hidden_size1,\n",
    "                          args.hidden_size2, args.output_size, args)\n",
    "\n",
    "        self.optimiser = th.optim.Adam(self.q_eval.parameters(), lr=args.lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        self._setup_checkpoints_dir(args)\n",
    "\n",
    "    def _setup_checkpoints_dir(self, args):\n",
    "        model_name = args.save_name\n",
    "        save_dir = \"./checkpoints/\"\n",
    "        self.chkpts_path = os.path.join(save_dir, model_name)\n",
    "        if not os.path.isdir(self.chkpts_path):\n",
    "            os.makedirs(self.chkpts_path)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # no need to accumulate gradients on inference, and memory stored\n",
    "            # as numpy ndarrays anyway\n",
    "            q_vals = self.q_eval.forward(observation).detach().cpu().numpy()\n",
    "            actions = np.argmax(q_vals, axis=1)\n",
    "        else:\n",
    "            actions = np.random.choice(\n",
    "                self.action_space, size=observation.shape[0])\n",
    "\n",
    "        return actions\n",
    "\n",
    "#     def store_transition(self, state, action, reward, state_, done):\n",
    "#         self.memory.store_transition(state, action, reward, state_, done)\n",
    "    # NOTE: too cumbersome to handle here now, may come back later\n",
    "\n",
    "#     def sample_memory(self):\n",
    "#         state, action, reward, new_state, done = \\\n",
    "#             self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "#         states = th.tensor(state).to(self.q_eval.device)\n",
    "#         rewards = th.tensor(reward).to(self.q_eval.device)\n",
    "#         dones = th.tensor(done).to(self.q_eval.device)\n",
    "#         actions = th.tensor(action).to(self.q_eval.device)\n",
    "#         states_ = th.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "#         return states, actions, rewards, states_, dones\n",
    "\n",
    "    def sample_memory(self):\n",
    "#         indices = np.random.choice(len(self.memory), self.args.batch_size)\n",
    "        batch = random.choices(self.memory, k=self.args.batch_size)\n",
    "        # Essentially just unpacking the experience\n",
    "        obs = th.from_numpy(\n",
    "            np.stack([ex.obs for ex in batch])).to(self.device)\n",
    "        rewards = th.from_numpy(\n",
    "            np.array([ex.reward for ex in batch],\n",
    "                     dtype=np.float32).reshape(-1, 1)\n",
    "        ).to(self.device)\n",
    "        dones = th.from_numpy(\n",
    "            np.array([ex.done for ex in batch],\n",
    "                     dtype=np.float32).reshape(-1, 1)\n",
    "        ).to(self.device)\n",
    "        dones = th.gt(dones, 0) # cast to bool tensor\n",
    "        actions = th.from_numpy(\n",
    "            np.stack([ex.action for ex in batch])).to(self.device)\n",
    "        next_obs = th.from_numpy(\n",
    "            np.stack([ex.next_obs for ex in batch])).to(self.device)\n",
    "\n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_checkpoints(self, num):\n",
    "        th.save(self.q_next.state_dict(),\n",
    "                os.path.join(self.chkpts_path, f\"qnext_{num}.pt\"))\n",
    "        th.save(self.q_eval.state_dict(),\n",
    "                os.path.join(self.chkpts_path, f\"qeval_{num}.pt\"))\n",
    "\n",
    "    def load_checkpoints(self, num):\n",
    "        self.q_next.load_state_dict(\n",
    "            th.load(os.path.join(self.chkpts_path, f\"qnext_{num}.pt\")))\n",
    "        self.q_eval.load_state_dict(\n",
    "            th.load(os.path.join(self.chkpts_path, f\"qnext_{num}.pt\")))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
    "        q_next = self.q_next.forward(states_).max(dim=1)[0]\n",
    "\n",
    "        dones = th.squeeze(dones, 1)\n",
    "#         print(q_next.shape)\n",
    "#         print(dones.shape)\n",
    "#         print(dones)\n",
    "        q_next[dones] = 0.0\n",
    "#         print(q_next)\n",
    "        q_target = rewards + self.gamma * q_next\n",
    "\n",
    "        loss = self.loss(q_target, q_pred).to(self.device)\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9e25b",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "531fd5bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:39.843407Z",
     "start_time": "2021-10-22T15:20:39.833204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(batch_size=32,\n",
       "          device='cuda:2',\n",
       "          epsilon=1.0,\n",
       "          gamma=0.99,\n",
       "          save_name='dqn',\n",
       "          lr=0.0001,\n",
       "          mem_size=50000,\n",
       "          eps_min=0.1,\n",
       "          eps_dec=0.0001,\n",
       "          replace=1000,\n",
       "          input_shape=361,\n",
       "          hidden_size1=256,\n",
       "          hidden_size2=128,\n",
       "          output_size=7)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = SimpleNamespace()\n",
    "\n",
    "args.batch_size = 32\n",
    "args.device = 'cuda:2'\n",
    "args.epsilon = 1.0\n",
    "args.gamma = 0.99\n",
    "args.save_name = 'dqn'\n",
    "args.lr = 0.0001\n",
    "args.mem_size = 50000\n",
    "args.eps_min = 0.1\n",
    "args.eps_dec = 1e-4\n",
    "args.replace = 1000\n",
    "# args.input_shape = (decision_steps.obs[1].shape[0],\n",
    "#                     decision_steps.obs[1].shape[1] + decision_steps.obs[2].shape[1])\n",
    "args.input_shape = decision_steps.obs[1].shape[1] + decision_steps.obs[2].shape[1]\n",
    "args.hidden_size1 = 256\n",
    "args.hidden_size2 = 128\n",
    "args.output_size = spec.action_spec.discrete_branches[0]\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13b886",
   "metadata": {},
   "source": [
    "# Playing in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22cfcc62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:40.612688Z",
     "start_time": "2021-10-22T15:20:40.609028Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33cb95",
   "metadata": {},
   "source": [
    "Need to find some way to handle cummulative rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b07e350f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T15:20:40.891926Z",
     "start_time": "2021-10-22T15:20:40.888047Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf94cd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-22T15:20:41.348Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cada0d67225242dc83d064228a30c138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "At game 0, epsilon=1.0\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "At game 25, epsilon=0.9692000000000034\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Internal: JobTempAlloc has allocations that are more than 4 frames old - this is not allowed and likely a leak\n",
      "\n",
      "Internal: deleting an allocation that is older than its permitted lifetime of 4 frames (age = 7)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "At game 50, epsilon=0.7351000000000292\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "At game 75, epsilon=0.6135000000000426\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Unlocked Door\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "At game 100, epsilon=0.5118000000000538\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "At game 125, epsilon=0.3065000000000764\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (1)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Picked up key\n",
      "\n",
      "Dragon ate DungeonEscapeAgent (2)\n",
      "\n",
      "Picked up key\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_score = -np.inf\n",
    "load_checkpoint = False\n",
    "n_games = 750\n",
    "\n",
    "agent = DQNAgent(args, env)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_checkpoints(num=0) # Placeholder\n",
    "\n",
    "dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "for i in trange(n_games):\n",
    "    done = False\n",
    "    env.reset()\n",
    "\n",
    "    score = 0\n",
    "    while not done:\n",
    "        # get observations somehow\n",
    "        decision_steps, terminal_steps = env.get_steps(agent.behavior_name)\n",
    "        # --------- Unsure about this, this does things in batch manner -----------\n",
    "#         ray_obs = th.tensor(decision_steps.obs[1]).to(agent.device)\n",
    "#         key_obs = th.tensor(decision_steps.obs[2]).to(agent.device)\n",
    "        # combine observations into 1 vector\n",
    "        obs = np.concatenate(\n",
    "            (decision_steps.obs[1], decision_steps.obs[2]), axis=1)\n",
    "        # -------------------------------------------------------------------------\n",
    "\n",
    "        # For all Agents with a Terminal Step:\n",
    "        for agent_id_terminated in terminal_steps:\n",
    "            # Create its last experience (is last because the Agent terminated)\n",
    "            last_experience = Experience(\n",
    "                obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "                reward=terminal_steps[agent_id_terminated].group_reward,\n",
    "                done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "                action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "                next_obs=np.concatenate(\n",
    "                    (terminal_steps[agent_id_terminated].obs[1],\n",
    "                     terminal_steps[agent_id_terminated].obs[2]), axis=0)\n",
    "            )\n",
    "            # Clear its last observation and action (Since the trajectory is over)\n",
    "            dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "            dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "\n",
    "            cumulative_reward = (\n",
    "                dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "                + terminal_steps[agent_id_terminated].group_reward\n",
    "            )\n",
    "            cumulative_rewards.append(cumulative_reward)\n",
    "\n",
    "            agent.memory.extend(dict_trajectories_from_agent.pop(\n",
    "                agent_id_terminated))\n",
    "            agent.memory.append(last_experience)\n",
    "\n",
    "        # For all Agents with a Decision Step:\n",
    "        for agent_id_decisions in decision_steps:\n",
    "            # If the Agent does not have a Trajectory, create an empty one\n",
    "            if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "                dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "                dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "            # If the Agent requesting a decision has a \"last observation\"\n",
    "            if agent_id_decisions in dict_last_obs_from_agent:\n",
    "                # Create an Experience from the last observation and the Decision Step\n",
    "                exp = Experience(\n",
    "                    obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "                    reward=decision_steps[agent_id_decisions].group_reward,\n",
    "                    done=False,\n",
    "                    action=dict_last_action_from_agent[agent_id_decisions].copy(\n",
    "                    ),\n",
    "                    next_obs=np.concatenate((decision_steps[agent_id_decisions].obs[1],\n",
    "                                            decision_steps[agent_id_decisions].obs[2]), axis=0)\n",
    "                )\n",
    "#                 print(exp.next_obs)\n",
    "                # Update the Trajectory of the Agent and its cumulative reward\n",
    "                dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "                dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "                    decision_steps[agent_id_decisions].group_reward\n",
    "                )\n",
    "#             print(decision_steps[agent_id_decisions].obs[1].shape)\n",
    "#             print(decision_steps[agent_id_decisions].obs[2].shape)\n",
    "            dict_last_obs_from_agent[agent_id_decisions] = np.concatenate((decision_steps[agent_id_decisions].obs[1],\n",
    "                                                                           decision_steps[agent_id_decisions].obs[2]), axis=0)\n",
    "\n",
    "        # when passing to agent in \"live\" env, bs is 1\n",
    "        # number of actions obtained = number of agents remaining alive\n",
    "        with th.no_grad():\n",
    "            # performing inference, don't want to accumulate gradients\n",
    "            actions = agent.choose_action(\n",
    "                th.from_numpy(obs).to(agent.device))\n",
    "        actions.resize(len(decision_steps), 1)\n",
    "        # Store the action that was picked, it will be put in the trajectory later\n",
    "        for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "            dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "        # Set the actions in the environment\n",
    "        # Unity Environments expect ActionTuple instances.\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(actions)\n",
    "#         print(action_tuple._discrete.shape)\n",
    "        env.set_actions(agent.behavior_name, action_tuple)\n",
    "        env.step()\n",
    "        \n",
    "        agent.learn()\n",
    "        \n",
    "        if len(decision_steps) == 0:\n",
    "            done = True\n",
    "        \n",
    "    if i % 25 == 0 or i == n_games-1:\n",
    "        print(f'At game {i}, epsilon={agent.epsilon}')\n",
    "        agent.save_checkpoints(num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3716682c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T16:55:20.467599Z",
     "start_time": "2021-10-22T16:55:20.349548Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104\n",
      "(array([ 246,  247,  425,  426,  514,  515,  540,  606,  607,  672,  673,\n",
      "        857,  858, 1082, 1083, 1326, 1650, 1651, 1822, 1823, 1857, 1858]),)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(cumulative_rewards))\n",
    "plt.plot(np.arange(len(cumulative_rewards)), cumulative_rewards)\n",
    "plt.title('Cumulative Group Rewards')\n",
    "plt.ylabel('rewards')\n",
    "plt.xlabel('agent terminations')\n",
    "plt.savefig('./plots/dqn_cum_rewards.png')\n",
    "print(np.nonzero(cumulative_rewards))\n",
    "\n",
    "np.save('./results/dqn_cum_rewards.npy', cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e11de03a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T16:50:57.368450Z",
     "start_time": "2021-10-22T16:50:57.345294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "rewards = [memory.reward for memory in agent.memory]\n",
    "print(np.nonzero(rewards)) # all positive rewards got pushed out of this buffer\n",
    "print(len(agent.memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fff4c2",
   "metadata": {},
   "source": [
    "# Wrapping Up, Closing the Environment\n",
    "Without closing the environment, we cannot make the environment again. Unity will complain about the communicator already being in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "486ab065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T16:52:56.907855Z",
     "start_time": "2021-10-22T16:52:56.688516Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 10 worker threads for Enlighten.\n",
      "  Thread -> id: 7f89e25b9700 -> priority: 1 \n",
      "  Thread -> id: 7f89e1db8700 -> priority: 1 \n",
      "  Thread -> id: 7f89e15b7700 -> priority: 1 \n",
      "  Thread -> id: 7f89e0db6700 -> priority: 1 \n",
      "  Thread -> id: 7f8957fff700 -> priority: 1 \n",
      "  Thread -> id: 7f89577fe700 -> priority: 1 \n",
      "  Thread -> id: 7f8956ffd700 -> priority: 1 \n",
      "  Thread -> id: 7f89567fc700 -> priority: 1 \n",
      "  Thread -> id: 7f8955ffb700 -> priority: 1 \n",
      "  Thread -> id: 7f89557fa700 -> priority: 1 \n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd2120",
   "metadata": {},
   "source": [
    "# QMIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72174c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlagents]",
   "language": "python",
   "name": "conda-env-mlagents-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "632px",
    "left": "532px",
    "top": "110px",
    "width": "221.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
